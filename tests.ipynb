{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-probability nb_black gin-config\n",
    "# !pip install torch==1.8.0+cpu torchvision==0.9.0+cpu torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.1268, -1.1092, -0.7310, -1.3338,  0.1591],\n",
      "         [ 0.3222,  1.5026, -0.6778,  1.4786,  0.2953],\n",
      "         [ 1.9041,  0.2432, -0.4155, -0.3035,  0.6855]]])\n",
      "tensor([[[3, 1, 2, 4, 0],\n",
      "         [2, 4, 0, 3, 1],\n",
      "         [2, 3, 1, 4, 0]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from perturbations_torch.fenchel_young import FenchelYoungLoss\n",
    "\n",
    "def ranks(inputs, dim=-1):\n",
    "    \"\"\"Returns the ranks of the input values among the given axis.\"\"\"\n",
    "    return 1 + inputs.argsort(dim).argsort(dim).type(inputs.dtype)\n",
    "\n",
    "x = torch.randn([3, 5]).float().unsqueeze(0)\n",
    "print(x)\n",
    "x.requires_grad = True\n",
    "y_true = torch.arange(5).float().unsqueeze(0).repeat([x.shape[0], 1])\n",
    "print(x.argsort(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD([x], 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[3, 1, 2, 4, 0],\n",
      "         [2, 0, 4, 1, 3],\n",
      "         [2, 3, 1, 4, 0]]])\n",
      "92.51268005371094\n",
      "tensor([[[1, 2, 3, 0, 4],\n",
      "         [0, 2, 1, 3, 4],\n",
      "         [0, 1, 2, 3, 4]]])\n",
      "29.038894653320312\n",
      "tensor([[[0, 1, 2, 3, 4],\n",
      "         [0, 1, 2, 3, 4],\n",
      "         [0, 1, 2, 3, 4]]])\n",
      "15.000822067260742\n",
      "tensor([[[0, 1, 2, 3, 4],\n",
      "         [0, 1, 2, 3, 4],\n",
      "         [0, 1, 2, 3, 4]]])\n",
      "15.000313758850098\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(200):\n",
    "    optim.zero_grad()\n",
    "    criterion = FenchelYoungLoss(ranks)\n",
    "    loss = criterion(y_true, x).sum()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if iteration % 50 == 0:\n",
    "        print(x.argsort(-1))\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.4375, -2.3969, -2.3266, -2.2855, -1.4416],\n",
       "         [-2.0240, -1.5973, -1.5565, -0.9716, -0.9297],\n",
       "         [-1.7595, -1.7227, -1.6877, -1.6510, -1.0653]]], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.any(((torch.sign(x + 2) + 1) / 2).bool(), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perturbations_torch import perturbations\n",
    "\n",
    "\n",
    "def reduce_sign_any(input_tensor, axis=-1):\n",
    "    \"\"\"A logical or of the signs of a tensor along an axis.\n",
    "\n",
    "  Args:\n",
    "   input_tensor: Tensor<float> of any shape.\n",
    "   axis: the axis along which we want to compute a logical or of the signs of\n",
    "     the values.\n",
    "\n",
    "  Returns:\n",
    "   A Tensor<float>, which as the same shape as the input tensor, but without the\n",
    "    axis on which we reduced.\n",
    "  \"\"\"\n",
    "    boolean_sign = tf.math.reduce_any(\n",
    "        tf.cast((tf.sign(input_tensor) + 1) / 2.0, dtype=tf.bool), axis=axis\n",
    "    )\n",
    "    return tf.cast(boolean_sign, dtype=input_tensor.dtype) * 2.0 - 1.0\n",
    "\n",
    "\n",
    "class PerturbationsTest(parameterized.TestCase, tf.test.TestCase):\n",
    "    \"\"\"Testing the perturbations module.\"\"\"\n",
    "\n",
    "    def setUp(self):\n",
    "        super(PerturbationsTest, self).setUp()\n",
    "        tf.random.set_seed(0)\n",
    "\n",
    "    @parameterized.parameters([perturbations._GUMBEL, perturbations._NORMAL])\n",
    "    def test_sample_noise_with_gradients(self, noise):\n",
    "        shape = (3, 2, 4)\n",
    "        samples, gradients = perturbations.sample_noise_with_gradients(\n",
    "            noise, shape\n",
    "        )\n",
    "        self.assertAllEqual(samples.shape, shape)\n",
    "        self.assertAllEqual(gradients.shape, shape)\n",
    "\n",
    "    def test_sample_noise_with_gradients_raise(self):\n",
    "        with self.assertRaises(ValueError):\n",
    "            _, _ = perturbations.sample_noise_with_gradients(\n",
    "                \"unknown\", (3, 2, 4)\n",
    "            )\n",
    "\n",
    "    @parameterized.parameters([1e-3, 1e-2, 1e-1])\n",
    "    def test_perturbed_reduce_sign_any(self, sigma):\n",
    "        input_tensor = tf.constant([[-0.3, -1.2, 1.6], [-0.4, -2.4, -1.0]])\n",
    "        soft_reduce_any = perturbations.perturbed(reduce_sign_any, sigma=sigma)\n",
    "        output_tensor = soft_reduce_any(input_tensor, axis=-1)\n",
    "        self.assertAllClose(output_tensor, [1.0, -1.0])\n",
    "\n",
    "    def test_perturbed_reduce_sign_any_gradients(self):\n",
    "        # We choose a point where the gradient should be above noise, that is\n",
    "        # to say the distance to 0 along one direction is about sigma.\n",
    "        sigma = 0.1\n",
    "        input_tensor = tf.constant(\n",
    "            [[-0.6, -1.2, 0.5 * sigma], [-2 * sigma, -2.4, -1.0]]\n",
    "        )\n",
    "        soft_reduce_any = perturbations.perturbed(reduce_sign_any, sigma=sigma)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(input_tensor)\n",
    "            output_tensor = soft_reduce_any(input_tensor)\n",
    "        gradient = tape.gradient(output_tensor, input_tensor)\n",
    "        # The two values that could change the soft logical or should be the one\n",
    "        # with real positive impact on the final values.\n",
    "        self.assertAllGreater(gradient[0, 2], 0.0)\n",
    "        self.assertAllGreater(gradient[1, 0], 0.0)\n",
    "        # The value that is more on the fence should bring more gradient than any\n",
    "        # other one.\n",
    "        self.assertAllLessEqual(gradient, gradient[0, 2].numpy())\n",
    "\n",
    "    def test_unbatched_rank_one_raise(self):\n",
    "        with self.assertRaises(ValueError):\n",
    "            input_tensor = tf.constant([-0.6, -0.5, 0.5])\n",
    "            dim = len(input_tensor)\n",
    "            n = 10000000\n",
    "\n",
    "            argmax = lambda t: tf.one_hot(tf.argmax(t, 1), dim)\n",
    "            soft_argmax = perturbations.perturbed(\n",
    "                argmax, sigma=0.5, num_samples=n\n",
    "            )\n",
    "            _ = soft_argmax(input_tensor)\n",
    "\n",
    "    def test_perturbed_argmax_gradients_without_minibatch(self):\n",
    "        input_tensor = tf.constant([-0.6, -0.5, 0.5])\n",
    "        dim = len(input_tensor)\n",
    "        eps = 1e-2\n",
    "        n = 10000000\n",
    "\n",
    "        argmax = lambda t: tf.one_hot(tf.argmax(t, 1), dim)\n",
    "        soft_argmax = perturbations.perturbed(\n",
    "            argmax, sigma=0.5, num_samples=n, batched=False\n",
    "        )\n",
    "        norm_argmax = lambda t: tf.reduce_sum(tf.square(soft_argmax(t)))\n",
    "\n",
    "        w = tf.random.normal(input_tensor.shape)\n",
    "        w /= tf.linalg.norm(w)\n",
    "        var = tf.Variable(input_tensor)\n",
    "        with tf.GradientTape() as tape:\n",
    "            value = norm_argmax(var)\n",
    "\n",
    "        grad = tape.gradient(value, var)\n",
    "        grad = tf.reshape(grad, input_tensor.shape)\n",
    "\n",
    "        value_minus = norm_argmax(input_tensor - eps * w)\n",
    "        value_plus = norm_argmax(input_tensor + eps * w)\n",
    "\n",
    "        lhs = tf.reduce_sum(w * grad)\n",
    "        rhs = (value_plus - value_minus) * 1.0 / (2 * eps)\n",
    "        self.assertAllLess(tf.abs(lhs - rhs), 0.05)\n",
    "\n",
    "    def test_perturbed_argmax_gradients_with_minibatch(self):\n",
    "        input_tensor = tf.constant([[-0.6, -0.7, 0.5], [0.9, -0.6, -0.5]])\n",
    "        dim = len(input_tensor)\n",
    "        eps = 1e-2\n",
    "        n = 10000000\n",
    "\n",
    "        argmax = lambda t: tf.one_hot(tf.argmax(t, -1), dim)\n",
    "        soft_argmax = perturbations.perturbed(argmax, sigma=2.5, num_samples=n)\n",
    "        norm_argmax = lambda t: tf.reduce_sum(tf.square(soft_argmax(t)))\n",
    "\n",
    "        w = tf.random.normal(input_tensor.shape)\n",
    "        w /= tf.linalg.norm(w)\n",
    "        var = tf.Variable(input_tensor)\n",
    "        with tf.GradientTape() as tape:\n",
    "            value = norm_argmax(var)\n",
    "\n",
    "        grad = tape.gradient(value, var)\n",
    "        grad = tf.reshape(grad, input_tensor.shape)\n",
    "\n",
    "        value_minus = norm_argmax(input_tensor - eps * w)\n",
    "        value_plus = norm_argmax(input_tensor + eps * w)\n",
    "\n",
    "        lhs = tf.reduce_sum(w * grad)\n",
    "        rhs = (value_plus - value_minus) * 1.0 / (2 * eps)\n",
    "        self.assertAllLess(tf.abs(lhs - rhs), 0.05)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.enable_v2_behavior()\n",
    "    tf.test.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
